{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"MarkovModel.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"gxwx5Yt_76F1"},"source":["# Word Prediction using Markov Model"]},{"cell_type":"markdown","metadata":{"id":"JxrE7KGO76F5"},"source":["This notebook makes use of Markov model for word prediction. Specifically 2nd order Markov model is deployed here for next word prediction. As an example of the Markov chain, an attempt is made to generate a new song lyrics from a bunch of Eminem song lyrics."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jT8pmtAb78zg","executionInfo":{"status":"ok","timestamp":1633059610782,"user_tz":240,"elapsed":19423,"user":{"displayName":"Zhenyu Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14019023693353042165"}},"outputId":"cec6378b-f286-43de-cae6-92ffc913cae6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"4WZqjRtm76F5"},"source":["# Preamble\n","import string\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvVLkGs576F7"},"source":["# Path of the text file containing the training data\n","training_data_file = '/content/drive/MyDrive/Colab_Notebooks/Merck_NLP_Project/word-prediction/pubmed_abstract.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdeiGg0w76F7"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"CK0G3eiq76F7"},"source":["### Helper functions"]},{"cell_type":"code","metadata":{"id":"Fbjcoa1g76F8"},"source":["def remove_punctuation(sentence):\n","    return sentence.translate(str.maketrans('','', string.punctuation))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpO1Y2I376F8"},"source":["def add2dict(dictionary, key, value):\n","    if key not in dictionary:\n","        dictionary[key] = []\n","    dictionary[key].append(value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKesOqZA76F9"},"source":["def list2probabilitydict(given_list):\n","    probability_dict = {}\n","    given_list_length = len(given_list)\n","    for item in given_list:\n","        probability_dict[item] = probability_dict.get(item, 0) + 1\n","    for key, value in probability_dict.items():\n","        probability_dict[key] = value / given_list_length\n","    return probability_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2dF3mYy76F-"},"source":["initial_word = {}\n","second_word = {}\n","transitions = {}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EOk4--aS76F-"},"source":["### Training function"]},{"cell_type":"code","metadata":{"id":"vPjKiFbq76F-"},"source":["# Trains a Markov model based on the data in training_data_file\n","def train_markov_model():\n","    for line in open(training_data_file):\n","        tokens = remove_punctuation(line.rstrip().lower()).split()\n","        tokens_length = len(tokens)\n","        for i in range(tokens_length):\n","            token = tokens[i]\n","            if i == 0:\n","                initial_word[token] = initial_word.get(token, 0) + 1\n","            else:\n","                prev_token = tokens[i - 1]\n","                if i == tokens_length - 1:\n","                    add2dict(transitions, (prev_token, token), 'END')\n","                if i == 1:\n","                    add2dict(second_word, prev_token, token)\n","                else:\n","                    prev_prev_token = tokens[i - 2]\n","                    add2dict(transitions, (prev_prev_token, prev_token), token)\n","    \n","    # Normalize the distributions\n","    initial_word_total = sum(initial_word.values())\n","    for key, value in initial_word.items():\n","        initial_word[key] = value / initial_word_total\n","        \n","    for prev_word, next_word_list in second_word.items():\n","        second_word[prev_word] = list2probabilitydict(next_word_list)\n","        \n","    for word_pair, next_word_list in transitions.items():\n","        transitions[word_pair] = list2probabilitydict(next_word_list)\n","    \n","    print('Training successful.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ye6g5Ye376F_","executionInfo":{"status":"ok","timestamp":1633060418354,"user_tz":240,"elapsed":288,"user":{"displayName":"Zhenyu Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14019023693353042165"}},"outputId":"fb409723-48ed-4cfd-f247-b289c21fb032"},"source":["train_markov_model()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training successful.\n"]}]},{"cell_type":"markdown","metadata":{"id":"2bfURAAT76F_"},"source":["## Testing"]},{"cell_type":"markdown","metadata":{"id":"im25HzVT76GA"},"source":["### Test functions"]},{"cell_type":"code","metadata":{"id":"qXW6E4W9_nAE"},"source":["test_word = ['quantitative analysis', 'mass transitions', 'concentration range', 'flow rate', 'accuracy was', 'lower limit']\n","word0 = []\n","word1 = []\n","number_of_sentences = len(test_word)\n","for i in range(number_of_sentences):\n","  word0.append(test_word[i].split()[0])\n","  word1.append(test_word[i].split()[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UR_jB1-CSUg"},"source":["# Function to generate sample text\n","def generate():\n","    for i in range(number_of_sentences):\n","        sentence = []\n","        # Initial word\n","        word_0 = word0[i]\n","        sentence.append(word_0)\n","        # Second word\n","        word_1 = word1[i]\n","        sentence.append(word_1)\n","        # Subsequent words untill END\n","        j = 0\n","        while True:\n","            word_2 = sample_word(transitions[(word_0,word_1)])\n","            if word_2 == 'END':\n","                break\n","            sentence.append(word_2)\n","            word_0 = word_1\n","            word_1 = word_2\n","            j += 1\n","        print(' '.join(sentence))\n","        #print(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdGj1csv76GB"},"source":["### Testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gVSYBA876GB","executionInfo":{"status":"ok","timestamp":1633063028494,"user_tz":240,"elapsed":171,"user":{"displayName":"Zhenyu Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14019023693353042165"}},"outputId":"2e803ee9-f9f6-4d92-e0a8-493a07ce7a1c"},"source":["generate()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["quantitative analysis of plasma samples obtained following the oral administration of vardenafil in human plasma\n","mass transitions were mz 4893 3122 for vardenafil and the accuracy was within 127 in terms of relative error\n","concentration range of 02100 ngml with correlation coefficients or 0995\n","flow rate of 04 mlmin\n","accuracy was within 127 in terms of relative error\n","lower limit of quantitation was set at 02 ngml\n"]}]}]}